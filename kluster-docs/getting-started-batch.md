---
title: Getting started guide for Kluster.ai 
description: A getting started guide to show user how to submit Batch jobs fast
hide:
- footer
---

# Getting started guide

Welcome to the Kluster.ai getting started guide! This document will show you how to get started submitting Batch jobs quickly. If you are using the OpenAI API endpoints, then the Kluster.ai endpoints will be familiar. In the next sections, we’ll show you Curl and Python examples of how to locate your API key, define a collection of Batch jobs as a JSON lines file, upload the file to the Kluster.ai endpoint, invoke the chat completion end point, monitor progress of the Batch job, retrieve the result of the Batch job, list all Batch objects, and cancel a Batch request.

## 1. Get your API Key

Navigate to the [platform.kluster.ai](http://platform.kluster.ai) web app and select **API Keys** from the left hand menu. Create a new API key by specifying the API key name. You’ll need this to set the auth header in all of the API requests.


## 2. List supported models

`get https://api.kluster.ai/v1/models` - Lists the currently available models.

First, let’s use the models endpoint to list out the models that we support. Currently, only Meta-Llama-3.1-8B-Instruct, Meta-Llama-3.1-70B-Instruct, and Meta-Llama-3.1-405B-Instruct are supported. The response is a list of model objects. 

### Query the supported models
--- 
```json title="Query LLMs"
curl https://api.kluster.ai/v1/models \
  -H "Authorization: Bearer $API_KEY" 
```

### Returns
---
<div class="grid" markdown>
<div markdown>
**id** <font color="gray">string</font> - The model identifier, which can be referenced in the API endpoints.

**created** <font color="gray">integer</font> - The Unix timestamp (in seconds) when the model was created.

**object** <font color="gray">string</font> - The object type, which is always "model".

**owned_by** <font color="gray">string</font> - The organization that owns the model.

</div>
</div>



### Example
---
```json title="Query LLMs"
[
   {
      "id":"meta-llama/Meta-Llama-3.1-70B-Instruct",
      "object":"model",
      "created":"1970-01-01T00:00:00Z",
      "owned_by":"owner1"
   },
   {
      "id":"meta-llama/Meta-Llama-3.1-8B-Instruct",
      "object":"model",
      "created":"1970-01-01T00:00:00Z",
      "owned_by":"owner2"
   },
   {
      "id":"meta-llama/Meta-Llama-3.1-405B-Instruct",
      "object":"model",
      "created":"1970-01-01T00:00:00Z",
      "owned_by":"owner3"
   }
```

## 3. Create a Batch file with a collection of jobs

Create a [JSON Lines](https://jsonlines.org/) file containing a collection of `batch request input` objects. The body of each is a `chat completion` object with the endpoint `/v1/chat/completions`. Each request must include a unique `custom_id` which is used to reference results after the Batch job has completed. 

### Batch request input object
---
**custom_id** <font color="gray">string</font> - A developer-provided per-request id that will be used to match outputs to inputs. Must be unique for each request in a Batch.

**method** <font color="gray">string</font> - The HTTP method to be used for the request. Currently only **`POST`** is supported.

**url** <font color="gray">string</font> - The `/v1/chat/completions` API relative URL

### Request body object (chat completion object)
---
**messages** array <font color="red">Required</font>

- **Assistant message** <font color="gray">object</font>

    - **role** <font color="gray">string</font> <font color="red">Required</font> - The role of the messages author, in this case `assistant`.

    - **content** <font color="gray">string or array Optional</font> - The contents of the assistant message.  

    - **refusal** <font color="gray">string or null Optional</font> - The refusal message by the assistant.
    
    - **name** string <font color="gray">Optional</font> - An optional name for the participant. Provides the model information to differentiate between participants of the same role.

   <details>
   <summary><strong>tool_calls</strong> <font color="gray">array Optional</font> </summary>

  - **id** <font color="gray">string</font> <font color="red">Required</font> - The ID of the tool call.

  - **type** <font color="gray">string</font>  <font color="red">Required</font> - The type of the tool. Currently, only function is supported.

    - **function** object <font color="red">Required</font> - The function that the model called.

      - **name** <font color="gray">string</font> <font color="red">Required</font> - The name of the function to call.

      - **arguments** <font color="gray">string</font> <font color="red">Required</font> - The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
 
</details>

- **tool message** object <font color="gray">Optional</font> 

    - **role** <font color="gray">string</font> <font color="red">Required</font> - The role of the messages author.
  
    - **content** <font color="gray">string or array</font> <font color="red">Required</font> - The contents of the tool message.

    - **tool_call_id** <font color="gray">string</font> <font color="red">Required</font> - Tool call that this message is responding to.


**model** <font color="gray">string</font> <font color="red">Required</font> - ID of the model to use. See the model endpoint compatibility table for details on which models work with the Chat API.

**store** <font color="gray">boolean or null Optional Defaults to false</font> - Whether or not to store the output of this chat completion request for use in our model distillation or evals products.

**metadata** <font color="gray">object or null Optional</font> - Developer-defined tags and values used for filtering completions in the dashboard.

**frequency_penalty** <font color="gray">number or null Optional Defaults to 0</font>  - Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.

**logit_bias** <font color="gray">map Optional Defaults to null </font> - Modify the likelihood of specified tokens appearing in the completion.

Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.

**logprobs** <font color="gray">boolean or null Optional Defaults to false </font> - Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`.

**top_logprobs** <font color="gray">integer or null Optional </font>- An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.

**max_completion_tokens** <font color="gray">integer or null Optional </font> - An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.

**n** <font color="gray">integer or null Optional Defaults to 1 </font> - The number of chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.

**presence_penalty** <font color="gray">number or null Optional Defaults to 0 </font> - Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.

**response_format** <font color="gray">object Optional </font> - An object specifying the format that the model must output. Compatible with Meta-Llama-3.1-8B-Instruct, Meta-Llama-3.1-70B-Instruct, and Meta-Llama-3.1-405B-Instruct.

Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema. 

Setting to `{ "type": "json_object" }` enables JSON mode, which ensures the message the model generates is valid JSON.

**Important:** when using JSON mode, you **must** also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly "stuck" request. Also note that the message content may be partially cut off if `finish_reason="length"`, which indicates the generation exceeded `max_tokens` or the conversation exceeded the max context length.

**seed** <font color="gray">integer or null Optional </font> - This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.

**service_tier** <font color="gray">string or null Optional Defaults to null</font> - Specifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:

- If set to 'auto', and the Project is Scale tier enabled, the system will utilize scale tier credits until they are exhausted.
- If set to 'auto', and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.
- If set to 'default', the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.
- When not set, the default behavior is 'auto'.
- When this parameter is set, the response body will include the service_tier utilized.

**stop** <font color="gray">string / array / null Optional Defaults to null </font> - Up to 4 sequences where the API will stop generating further tokens.

**stream** <font color="gray">boolean or null Optional Defaults to false </font> - If set, partial message deltas will be sent, like in ChatGPTpl. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a `data: [DONE]` message.

**stream_options** <font color="gray">object or null  Optional Defaults to null </font> - Options for streaming response. Only set this when you set `stream: true` .

**temperature** <font color="gray">number or null Optional Defaults to 1 </font> - The sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.

We generally recommend altering this or `top_p` but not both.

**top_p** <font color="gray">number or null Optional Defaults to 1 </font> - An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.

We generally recommend altering this or `temperature` but not both.

**tools** <font color="gray">array Optional </font> - A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.
- **type** <font color="gray">string</font> <font color="red">Required</font> - The type of the tool. Currently, only function is supported.

- **function** <font color="gray">object</font> <font color="red">Required</font>

    - **description** <font color="gray">string Optional</font> - A description of what the function does, used by the model to choose when and how to call the function.

    - **name** <font color="gray">string</font> <font color="red">Required</font> - The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
    
    - **parameters**  <font color="gray">object Optional</font> - The parameters the functions accepts, described as a JSON Schema object. 
    
    Omitting `parameters` defines a function with an empty parameter list.
    
    - **strict** <font color="gray">boolean or null Optional Defaults to false</font> - Whether to enable strict schema adherence when generating the function call. If set to true, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is true. Learn more about Structured Outputs in the function calling guide.

**tool_choice** <font color="gray">string or object Optional</font> - Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via `{"type": "function", "function": {"name": "my_function"}}` forces the model to call that tool.

  `none` is the default when no tools are present. `auto` is the default if tools are present.

  **parallel_tool_calls** <font color="gray">boolean Optional Defaults to true</font> - Whether to enable parallel function calling during tool use.

**user** <font color="gray">string</font> Optional </font>- A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.

### Returns
---
Returns a chat completion object.


### Curl example
Here's an example of an input file with 2 Batch requests.

```json title="Collection of Batch jobs"
{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "meta-llama/Meta-Llama-3.1-8B-Instruct", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "What is the capital of Argentina?"}],"max_tokens":1000}}
{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "meta-llama/Meta-Llama-3.1-70B-Instruct", "messages": [{"role": "system", "content": "You are an experienced maths tutor."}, {"role": "user", "content": "Explain the Pythagorean theorem to a 10 year old child"}],"max_tokens":1000}}
{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "meta-llama/Meta-Llama-3.1-405B-Instruct", "messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "What is the distance between the Earth and the Moon"}],"max_tokens":1000}}
```

### Python example
```Json
# Sample tasks for batch inference
tasks = [
    {
        "custom_id": "request-1",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "meta-llama/Meta-Llama-3.1-8B-Instruct",
            "messages": [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": "What is the capital of Argentina?"},
            ],
            "max_tokens": 1000,
        },
    },
    {
        "custom_id": "request-2",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "meta-llama/Meta-Llama-3.1-70B-Instruct",
            "messages": [
                {"role": "system", "content": "You are a maths tutor."},
                {"role": "user", "content": "You are an experienced maths tutor."}, {"role": "user", "content": "Explain the Pythagorean theorem to a 10 year old child."},
            ],
            "max_tokens": 1000,
        },
    },
    {
        "custom_id": "request-3",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "meta-llama/Meta-Llama-3.1-405B-Instruct",
            "messages": [
                {"role": "system", "content": "You are a maths tutor."},
                {"role": "user", "content": "You are an experienced maths tutor."}, {"role": "user", "content": "What is the distance between the Earth and the Moon."},
            ],
            "max_tokens": 1000,
        },
    }
    # Additional tasks can be added here
]

# Save tasks to a JSONL file (newline-delimited JSON)
file_name = "batch_tasks.jsonl"
with open(file_name, "w") as file:
    for task in tasks:
        file.write(json.dumps(task) + "\n")
```


## 4. Upload your Batch input file
`https://api.kluster.ai/v1/files`

Upload a [JSON Lines](https://jsonlines.org/) document to the Kluster.ai endpoint and take a note of the `id` in the response object.


### Request body
---
**file** <font color="gray">file</font> <font color="red">Required</font> - The File object (not file name) to be uploaded.

**purpose** <font color="gray">string</font> <font color="red">Required</font> - The intended purpose of the uploaded file. Use `batch` for the Batch API

### Returns

The uploaded [**File**](link to Kluster file object description!!) object.

**id** <font color="gray">string</font> - The file identifier, which can be referenced in the API endpoints.

**object** <font color="gray">string</font> - The object type, which is always `file`.

**bytes** <font color="gray">integer</font> - The size of the file, in bytes.

**created_at** <font color="gray">integer</font> - The Unix timestamp (in seconds) for when the file was created.

**filename** <font color="gray">string</font> - The name of the file.

**purpose** <font color="gray">string</font> - The intended purpose of the file. Currently, only  `batch` is supported.

### File upload request (Curl)
---
```Json
curl -s https://api.kluster.ai/v1/files \
-H "Authorization: Bearer $API_KEY" \
-H "Content-Type: multipart/form-data" \
-F "file=@mybatchtest.jsonl" \
-F "purpose=batch"
```

### File upload request (Python)
```Json
# Upload the file to the platform
batch_file = client.files.create(file=open(file_name, "rb"), purpose="batch")
print(f"Batch file uploaded. File ID: {batch_file.id}")
```

### File upload response
---
```Json
{
"id": "6d1a6cb8-ca57-4465-8649-26e93f7e0ca8",
"object": "file",
"bytes": 987,
"created_at": "2024-10-07T12:51:15.725288678Z",
"filename": "myBatchTasks2.jsonl",
"purpose": "batch"
}
```

## 5. Invoke the chat completion end point
`post https://api.kluster.ai/v1/batches`

Next, to create a Batch job, you invoke the chat completion API using the `input_file_id` from the previous step.

### Request body
---
**input_file_id** <font color="gray">string</font> <font color="red">Required</font> - The ID of an uploaded file that contains requests for the new Batch. 

Your input file must be formatted as a [**JSONL file**](EH add link), and must be uploaded with the purpose `batch`. The file can contain up to 50,000 requests, and can be up to 100 MB in size.

**endpoint** <font color="gray">string</font> <font color="red">Required</font> - The endpoint to be used for all requests in the Batch. Currently, only`/v1/chat/completions` ****is supported.

**completion_window** <font color="gray">string</font> <font color="red">Required</font> - The time frame within which the Batch should be processed. Currently only **`24h`** is supported.

**metadata** <font color="gray">Object or null</font> <font color="gray">Optional</font> - Optional custom metadata for the Batch.

### Returns
---
The created [Batch](#batch-object) object.

**id** <font color="gray">string</font> 

**object** <font color="gray">string</font> - The object type, which is always `batch`.

**endpoint** <font color="gray">string</font> - The Kluster.ai API endpoint used by the batch.

**errors** <font color="gray">object</font> - Show properties

**input_file_id** <font color="gray">string</font> - The ID of the input file for the batch.

**completion_window** <font color="gray">string</font> - The time frame within which the batch should be processed.

**status** <font color="gray">string</font> - The current status of the batch.

**output_file_id** <font color="gray">string</font> - The ID of the file containing the outputs of successfully executed requests.

**error_file_id** <font color="gray">string</font> - The ID of the file containing the outputs of requests with errors.

**created_at** <font color="gray">integer</font> - The Unix timestamp (in seconds) for when the Batch was created.

**in_progress_at** <font color="gray">integer</font> - The Unix timestamp (in seconds) for when the Batch started processing.

**expires_at** <font color="gray">integer</font> - The Unix timestamp (in seconds) for when the Batch will expire.

**finalizing_at** <font color="gray">integer</font> - The Unix timestamp (in seconds) for when the Batch started finalizing.

**completed_at** <font color="gray">integer</font> - The Unix timestamp (in seconds) for when the Batch was completed.

**failed_at** <font color="gray">integer</font> - The Unix timestamp (in seconds) for when the Batch failed.

**expired_at** <font color="gray">integer</font> - The Unix timestamp (in seconds) for when the Batch expired.

**cancelling_at** <font color="gray">integer</font> - The Unix timestamp (in seconds) for when the Batch started cancelling.

**cancelled_at** <font color="gray">integer</font> - The Unix timestamp (in seconds) for when the Batch was cancelled.

**request_counts** <font color="gray">object</font> - The request counts for different statuses within the Batch.

**metadata** <font color="gray">map</font> - Set of 16 key-value pairs that can be attached to an object. This is useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maximum of 512 characters long.

### Create Batch job (Curl)
```json
curl -s https://api.kluster.ai/v1/batches \
-H "Authorization: Bearer $API_KEY" \
-H "Content-Type: application/json" \
-d '{
"input_file_id": "kluster-file-123",
"endpoint": "/v1/chat/completions",
"completion_window": "24h"
}')
```

### Create Batch job (Python)
```jSON
# Submit the batch request
batch_request = client.batches.create(
    input_file_id=batch_file.id,
    endpoint="/v1/chat/completions",
    completion_window="24h",  # Maximum time allowed for the batch to complete
    metadata={"description": "Batch job for chat completions"},
)

print(f"Batch request submitted. Batch ID: {batch_request.id}")
```

### Create Batch response

```json
{
  "id": "2b96e3e4-cd7b-43fd-9dd3-d82153bdd752",
  "object": "batch",
  "endpoint": "/v1/chat/completions",
  "errors": null,
  "input_file_id": "kluster-input-file-123",
  "completion_window": "24h",
  "status": "Validating",
  "output_file_id": null,
  "error_file_id": null,
  "created_at": "2024-10-07T12:45:38.109049154Z",
  "in_progress_at": null,
  "expires_at": "2024-10-08T12:45:38.109049154Z",
  "finalizing_at": null,
  "completed_at": null,
  "failed_at": null,
  "expired_at": null,
  "cancelling_at": null,
  "cancelled_at": null,
  "request_counts": {
    "total": 0,
    "completed": 0,
    "failed": 0
  },
  "metadata": {}
}
```

## 6. Monitor the progress of the Batch job

`get https://api.kluster.ai/v1/batches/{batch_id}`

To verify that the Batch job has finished, check the `status` property for `completed` using the Batch id from the previous step.

### Path parameters

---

**Batch_id** <font color="gray">string</font> <font color="red">Required</font> - The ID of the Batch to retrieve.

### Returns
---

The [**Batch**](EH update) object matching the specified `id`.

### Get Batch status (Curl)
```json
curl -s https://api.kluster.ai/v1/batches/2b96e3e4-cd7b-43fd-9dd3-d82153bdd752 \
  -H "Authorization: Bearer $API_KEY" \
  -H "Content-Type: application/json"
```

### Get Batch status (Python)
```Json
import time

# Poll the batch status until it's complete
while True:
    batch_status = client.batches.retrieve(batch_request.id)
    print(f"Batch status: {batch_status.status}")
    print(
        f"Completed tasks: {batch_status.request_counts.completed} / {batch_status.request_counts.total}"
    )

    if batch_status.status.lower() in ["completed", "failed", "canceled"]:
        break

    time.sleep(10)  # Wait for 10 seconds before checking again
```
### Response
```json
{
  "id": "2b96e3e4-cd7b-43fd-9dd3-d82153bdd752",
  "object": "batch",
  "endpoint": "/v1/chat/completions",
  "errors": null,
  "input_file_id": "kluster-input-file-123",
  "completion_window": "24h",
  "status": "Completed",
  "output_file_id": "kluster-output-file-123",
  "error_file_id": null,
  "created_at": "2024-10-07T13:08:52.821427Z",
  "in_progress_at": null,
  "expires_at": "2024-10-08T13:08:52.821427Z",
  "finalizing_at": null,
  "completed_at": null,
  "failed_at": null,
  "expired_at": null,
  "cancelling_at": null,
  "cancelled_at": null,
  "request_counts": {
    "total": 3,
    "completed": 3,
    "failed": 0
  },
  "metadata": {}
}
```

## 7. Retrieve the file content of the Batch job

`get https://api.kluster.ai/v1/files/{file_id}/content`

To retrieve the file content of the Batch job, send a request to the `files` end point specifying the `output_file_id` and redirecting standard output to a file.

### Path parameters

**file_id** <font color="gray">string</font> <font color="red">Required</font> - The ID of the file to use for this request.

### **Returns**

The output file content matching the specified file ID.

### Retrieve the results (Curl)
---
In this example, we retrieve the Batch output file and write it to a local file called `batch_output.jsonl`
```json
curl -s https://api.kluster.ai/v1/files/kluster-output-file-123"/content \
-H "Authorization: Bearer $API_KEY" > batch_output.jsonl

```

### Retrieve the results (Python)
```Json
# Check if the batch completed successfully
if batch_status.status.lower() == "completed":
    # Retrieve the results
    result_file_id = batch_status.output_file_id
    results = client.files.content(result_file_id).content

    # Save results to a file
    result_file_name = "data/batch_results.jsonl"
    with open(result_file_name, "wb") as file:
        file.write(results)
    print(f"Results saved to {result_file_name}")
else:
    print(f"Batch failed with status: {batch_status.status}")
```

## 8. List all Batch jobs

`get https://api.kluster.ai/v1/batches`

To list all of your Batch objects, send a request to the batches endpoint without specifying a batch_id. To constrain the query response, you can also use a limit parameter. 

### Query parameters
---

**after** <font color="gray">string</font> <font color="gray">Optional</font> - A cursor for use in pagination. `after` is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include after=obj_foo in order to fetch the next page of the list.

**limit** <font color="gray">integer</font> <font color="gray">Optional</font> - Defaults to 20

A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20.

### Returns

---
A list of paginated [**Batch**](https://www.notion.so/Getting-started-with-Curl-118511da207f8096a03fcab492c17213?pvs=21) objects.

### Query Batch endpoint (Curl)
```json
 curl https://api.kluster.ai/v1/batches \
  -H "Authorization: Bearer $API_KEY"
```

### Query Batch endpoint (Python)
```Json
# Configure OpenAI client
client = OpenAI(
    base_url="https://api.kluster.ai/v1",  # Change to "http://localhost:9000/api/v1" for local development
    api_key=my_api_key,  # Replace with your actual API key
)

print(client.batches.list(limit=2))
```

### Query batch endpoint response
```json
{
  "object": "list",
  "data": [
    {
      "id": "92aa978c-9dd1-49af-baa8-485ae6fb8019",
      "object": "batch",
      "endpoint": "/v1/chat/completions",
      "errors": null,
      "input_file_id": "kluster-input-file-123",
      "completion_window": "24h",
      "status": "Completed",
      "output_file_id": "kluster-output-file-123",
      "error_file_id": null,
      "created_at": "2024-10-07T16:53:53.046181Z",
      "in_progress_at": null,
      "expires_at": "2024-10-08T16:53:53.046181Z",
      "finalizing_at": null,
      "completed_at": null,
      "failed_at": null,
      "expired_at": null,
      "cancelling_at": null,
      "cancelled_at": null,
      "request_counts": {
        "total": 2,
        "completed": 2,
        "failed": 0
      },
      "metadata": {}
    },
   { ... },
  ],
  "first_id": "92aa978c-9dd1-49af-baa8-485ae6fb8019",
  "last_id": "0d29e406-51b2-4e5b-a9f4-6cb460eaeb59",
  "has_more": false,
  "count": 1,
  "page": 1,
  "page_count": -1,
  "items_per_page": 9223372036854775807
}
```

The status of a Batch object can one of the following

| Status	| Description |
|---------|-------------|
| `validating` |validating	the input file is being validated|
| `failed` |failed	the input file failed the validation process|
| `in_progress` |in_progress	the input file was successfully validated and the Batch is currently being in progress|
| `finalizing` |finalizing	the Batch job has completed and the results are being finalized|
| `completed` |completed	the Batch has completed and the results are ready|
| `expired` |expired	the Batch was not completed within the 24-hour time window|
| `cancelling` |cancelling	the Batch is being cancelled (may take up to 10 minutes)|
| `cancelled` |cancelled	the Batch was cancelled|

## 9. Cancelling a Batch job

`post https://api.kluster.ai/v1/batches/{batch_id}/cancel`

To cancel an in-progress Batch job, send a cancel request to the batches endpoint specifying the `batch_id`. 

### Path parameters

---

**batch_id** <font color="gray">string</font> <font color="red">Required</font> - The ID of the Batch to cancel.

#### Returns

---

The [**Batch**](EH update) object matching the specified ID.

### Cancel a batch request (Curl)

---
```json
curl -s https://api.kluster.ai/v1/batches/$BATCH_ID/cancel \
  -H "Authorization: Bearer $API_KEY" \
  -H "Content-Type: application/json" \
  -X POST
```

### Cancel a batch request (Python)
```Json
client = OpenAI(
    base_url="https://api.kluster.ai/v1",  
    api_key=my_api_key,  # Replace with your actual API key
)
client.batches.cancel("kluster_123")
```

### Cancel response
---
```json
{
  "id": "642853d4-e816-4be2-8453-6ce6f0f00b9c",
  "object": "batch",
  "endpoint": "/v1/chat/completions",
  "errors": null,
  "input_file_id": "kluster-input-file-123",
  "completion_window": "24h",
  "status": "Cancelling",
  "output_file_id": "kluster-output-file-123",
  "error_file_id": null,
  "created_at": "2024-10-07T17:15:09.885223Z",
  "in_progress_at": null,
  "expires_at": "2024-10-08T17:15:09.885223Z",
  "finalizing_at": null,
  "completed_at": null,
  "failed_at": null,
  "expired_at": null,
  "cancelling_at": "2024-10-07T17:15:17.934748Z",
  "cancelled_at": null,
  "request_counts": {
    "total": 3,
    "completed": 3,
    "failed": 0
  },
  "metadata": {}
}
```

## 10. Summary

You’ve now run a simple Batch use case by sending a collection of Batch request input objects to the chat completion end point, monitored the Batch interface for the progress of the job, and downloaded the result of the Batch job. To learn more about the end points we support, refer to the API documentation (link).

# API Reference

## Introduction
To interact with the Kluster.ai service, you can send HTTP requests or modify the endpoint in the OpenAI Python bindings or the Node.js library to `api.kluster.ai`. 

To install the Python bindings , run the following command:

```
pip install openai
```

To install the official Node.js library, run the following command in your Node.js project directory:

```
npm install openai
```

## Authentication

### API Keys

The Kluster.ai API uses API keys for authentication which create from `https://platform.kluster.ai/api-keys`. All API requests should include your API key in an Authorization HTTP header :

```
Authorization: Bearer $API_KEY
```

## Chat

## Create chat completion

`post https://api.openai.com/v1/chat/completions`

Creates a model response for the given chat conversation.

**messages** array <font color="red">Required</font>

- **Assistant message** <font color="gray">object</font>

    - **role** <font color="gray">string</font> <font color="red">Required</font> - The role of the messages author, in this case `assistant`.

    - **content** <font color="gray">string or array Optional</font> - The contents of the assistant message.  

    - **refusal** <font color="gray">string or null Optional</font> - The refusal message by the assistant.
    
    - **name** string <font color="gray">Optional</font> - An optional name for the participant. Provides the model information to differentiate between participants of the same role.

   <details>
   <summary><strong>tool_calls</strong> <font color="gray">array Optional</font> </summary>

  - **id** <font color="gray">string</font> <font color="red">Required</font> - The ID of the tool call.

  - **type** <font color="gray">string</font>  <font color="red">Required</font> - The type of the tool. Currently, only function is supported.

    - **function** object <font color="red">Required</font> - The function that the model called.

      - **name** <font color="gray">string</font> <font color="red">Required</font> - The name of the function to call.

      - **arguments** <font color="gray">string</font> <font color="red">Required</font> - The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
 
</details>

- **tool message** object <font color="gray">Optional</font> 

    - **role** <font color="gray">string</font> <font color="red">Required</font> - The role of the messages author.
  
    - **content** <font color="gray">string or array</font> <font color="red">Required</font> - The contents of the tool message.

    - **tool_call_id** <font color="gray">string</font> <font color="red">Required</font> - Tool call that this message is responding to.


**model** <font color="gray">string</font> <font color="red">Required</font> - ID of the model to use. See the model endpoint compatibility table for details on which models work with the Chat API.

**store** <font color="gray">boolean or null Optional Defaults to false</font> - Whether or not to store the output of this chat completion request for use in our model distillation or evals products.

**metadata** <font color="gray">object or null Optional</font> - Developer-defined tags and values used for filtering completions in the dashboard.

**frequency_penalty** <font color="gray">number or null Optional Defaults to 0</font>  - Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.

**logit_bias** <font color="gray">map Optional Defaults to null </font> - Modify the likelihood of specified tokens appearing in the completion.

Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.

**logprobs** <font color="gray">boolean or null Optional Defaults to false </font> - Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`.

**top_logprobs** <font color="gray">integer or null Optional </font>- An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.

**max_completion_tokens** <font color="gray">integer or null Optional </font> - An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.

**n** <font color="gray">integer or null Optional Defaults to 1 </font> - The number of chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.

**presence_penalty** <font color="gray">number or null Optional Defaults to 0 </font> - Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.

**response_format** <font color="gray">object Optional </font> - An object specifying the format that the model must output. Compatible with Meta-Llama-3.1-8B-Instruct, Meta-Llama-3.1-70B-Instruct, and Meta-Llama-3.1-405B-Instruct.

Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema. 

Setting to `{ "type": "json_object" }` enables JSON mode, which ensures the message the model generates is valid JSON.

**Important:** when using JSON mode, you **must** also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly "stuck" request. Also note that the message content may be partially cut off if `finish_reason="length"`, which indicates the generation exceeded `max_tokens` or the conversation exceeded the max context length.

**seed** <font color="gray">integer or null Optional </font> - This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.

**service_tier** <font color="gray">string or null Optional Defaults to null</font> - Specifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:

- If set to 'auto', and the Project is Scale tier enabled, the system will utilize scale tier credits until they are exhausted.
- If set to 'auto', and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.
- If set to 'default', the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.
- When not set, the default behavior is 'auto'.
- When this parameter is set, the response body will include the service_tier utilized.

**stop** <font color="gray">string / array / null Optional Defaults to null </font> - Up to 4 sequences where the API will stop generating further tokens.

**stream** <font color="gray">boolean or null Optional Defaults to false </font> - If set, partial message deltas will be sent, like in ChatGPTpl. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a `data: [DONE]` message.

**stream_options** <font color="gray">object or null  Optional Defaults to null </font> - Options for streaming response. Only set this when you set `stream: true` .

**temperature** <font color="gray">number or null Optional Defaults to 1 </font> - The sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.

We generally recommend altering this or `top_p` but not both.

**top_p** <font color="gray">number or null Optional Defaults to 1 </font> - An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.

We generally recommend altering this or `temperature` but not both.

**tools** <font color="gray">array Optional </font> - A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.
- **type** <font color="gray">string</font> <font color="red">Required</font> - The type of the tool. Currently, only function is supported.

- **function** <font color="gray">object</font> <font color="red">Required</font>

    - **description** <font color="gray">string Optional</font> - A description of what the function does, used by the model to choose when and how to call the function.

    - **name** <font color="gray">string</font> <font color="red">Required</font> - The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
    
    - **parameters**  <font color="gray">object Optional</font> - The parameters the functions accepts, described as a JSON Schema object. 
    
    Omitting `parameters` defines a function with an empty parameter list.
    
    - **strict** <font color="gray">boolean or null Optional Defaults to false</font> - Whether to enable strict schema adherence when generating the function call. If set to true, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is true. Learn more about Structured Outputs in the function calling guide.

**tool_choice** <font color="gray">string or object Optional</font> - Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via `{"type": "function", "function": {"name": "my_function"}}` forces the model to call that tool.

  `none` is the default when no tools are present. `auto` is the default if tools are present.

  **parallel_tool_calls** <font color="gray">boolean Optional Defaults to true</font> - Whether to enable parallel function calling during tool use.

**user** <font color="gray">string</font> Optional </font>- A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.

### Returns
---
Returns a chat completion object, or a streamed sequence of chat completion chunk objects if the request is streamed.

### Example (Curl)
```Json
curl https://api.kluster.ai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $API_KEY" \
  -d '{
    "model": "meta-llama/Meta-Llama-3.1-405B-Instruct",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "Explain the Pythagorean theorem to a 10 year old child"
      }
    ]
  }'
```

### Example (Python)
```Json

from openai import OpenAI

file_name="you-file-name" #replace with you filename

client = OpenAI(
api_key=my_api_key #replace with your API KEY
base_url="http://api.kluster.ai/v1"
)

batch_input_file = client.files.create(
  file=open(file_name, "rb"),
  purpose="batch"
)

print("batch file uploaded. File id= " + batch_input_file.id)

response = client.batches.create(
    input_file_id=batch_input_file.id
    endpoint="/v1/chat/completions",
    completion_window="24h",
    metadata={"description" : "batch job for chat completion"},
)

```

## Chat completion object

Represents a chat completion response returned by the LLM based on the provided input.

**id** <font color="gray">string</font> - A unique identifier for the chat completion.

**choices** <font color="gray">array</font> - A list of chat completion choices. Can be more than one if n is greater than 1. </summary> 

- **finish_reason** string The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool, or function_call (deprecated) if the model called a function.

- **index** <font color="gray">integer</font>  - The index of the choice in the list of choices.

- **message** <font color="gray">object</font>  - A chat completion message generated by the model.

  - **content** <font color="gray">array or null</font> - A list of message content tokens with log probability information.
   
  - **refusal** <font color="gray">array or null</font> - A list of message refusal tokens with log probability information.

  - **tool_calls** <font color="gray">array</font> - The tool calls generated by the model, such as function calls.

    - **id** <font color="gray">string</font> - The ID of the tool call.
    
    - **type** <font color="gray">string</font> - The type of the tool. Currently, only function is supported.
    
    - **function** <font color="gray">object</font> - The function that the model called

      - **name** <font color="gray">string</font> - The name of the function to call.
      
      - **arguments** <font color="gray">string</font> - The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function. 

  - **role** <font color="gray">string</font> - The role of the author of this message.

- **logprobs** <font color="gray">object or null</font> Log probability information for the choice.
  
  - **content** <font color="gray">array or null</font> - A list of message content tokens with log probability information.
   
  - **refusal** <font color="gray">array or null</font> - A list of message refusal tokens with log probability information.

**created** <font color="gray">integer</font> - The Unix timestamp (in seconds) of when the chat completion was created.

**model** <font color="gray">string</font> - The model used for the chat completion.

**service_tier** <font color="gray">string or null</font> - The service tier used for processing the request. This field is only included if the service_tier parameter is specified in the request.

**system_fingerprint** <font color="gray">string</font> - This fingerprint represents the backend configuration that the model runs with.

It can be used in conjunction with the seed request parameter to understand when backend changes have been made that might impact determinism.

**created** <font color="gray">integer</font> - The Unix timestamp (seconds) when the chat completion was created.

**model** <font color="gray">string</font> - The model used for the chat completion.

**service_tier** <font color="gray">string or null</font> - The service tier used for processing the request. This field is only included if the service_tier parameter is specified in the request.

**system_fingerprint** <font color="gray">string</font> - This fingerprint represents the backend configuration that the model runs with.

Can be used in conjunction with the seed request parameter to understand when backend changes have been made that might impact determinism.

**object** <font color="gray">string</font> - The object type, which is always chat.completion.

**usage** <font color="gray">object</font> - Usage statistics for the completion request.

- **completion_tokens** <font color="gray">integer</font> - Number of tokens in the generated completion.

- **prompt_tokens** <font color="gray">integer</font> - Number of tokens in the prompt.

- **total_tokens** <font color="gray">integer</font>  - Total number of tokens used in the request (prompt + completion). 

- **completion_tokens_details** <font color="gray">object</font> - Breakdown of tokens used in a completion.

  - **audio_tokens** <font color="gray">integer</font> - Audio input tokens generated by the model.

  - **reasoning_tokens** <font color="gray">integer</font> - Tokens generated by the model for reasoning.

- **prompt_tokens_details** <font color="gray">object</font> - Breakdown of tokens used in the prompt

  - **audio_tokens** <font color="gray">integer</font> - Audio input tokens present in the prompt.

  - **cached_tokens** <font color="gray">integer</font> - Cached tokens present in the prompt.

## Batch

## Create batch
`post https://api.kluster.ai/v1/batches`

Creates and executes a Batch based on the uploaded file of requests

### Request body
---
**input_file_id** <font color="gray">string</font> <font color="red">Required</font> - The ID of an uploaded file that contains requests for the new Batch. 

Your input file must be formatted as a [JSONL file](https://jsonlines.org/), and must be uploaded with the purpose set to `batch`. The file can contain up to 50,000 requests, and can be up to 100 MB in size.

**endpoint** <font color="gray">string</font> <font color="red">Required</font> - The endpoint to be used for all requests in the Batch. Currently, only `/v1/chat/completions` is supported.

**completion_window** <font color="gray">string</font> <font color="red">Required</font> - The time frame within which the Batch should be processed. Currently only **`24h`** is supported.

**metadata** <font color="gray">Object or null</font> <font color="gray">Optional</font> - Optional custom metadata for the Batch.

### Returns
---
The created [Batch](#batch-object) object.

## Retrieve batch

`get https://api.openai.com/v1/batches/{batch_id}`

Retrieves a Batch matching `batch_id`.

### Path parameters
---
**batch_id** <font color="gray">string</font> <font color="red">Required</font> - The ID of the batch to retrieve.

### Returns
---
The [Batch](#batch-object) object matching the specified ID.


## Cancel batch

`post https://api.kluster.ai/v1/batches/{batch_id}/cancel` 

Cancels an in-progress Batch job. Before changing to cancelled, the Batch will be in status cancelling for up to 10 minutes, after which it will have partial results (if any) available in the output file.

### Path parameters

---

**batch_id** <font color="gray">string</font> <font color="red">Required</font> - The ID of the Batch to cancel.

#### Returns

---

The [Batch](#batch-object) object matching the specified ID.


## List batch

`get https://api.openai.com/v1/batches` - Lists your Batch requests.

### Query parameters
---

**after** <font color="gray">string</font> <font color="gray">Optional</font> - A cursor for use in pagination `after` is an object ID that defines your place in the list. For example, if you make a list request and receive 100 objects, ending with kluster_foo, your subsequent call can include after=kluster_foo in order to fetch the next page of the list.

**limit**<font color="gray">integer</font> <font color="gray">Optional</font> - A limit on the number of objects returned which can range between 1 and 100. The default is 20.

### Returns
---
A list of paginated Batch objects.

## Batch object

**id** <font color="gray">string</font> 

**object** <font color="gray">string</font> - The object type, which is always `batch`.

**endpoint** <font color="gray">string</font> - The Kluster.ai API endpoint used by the batch.

**errors** <font color="gray">object</font> - Show properties

**input_file_id** <font color="gray">string</font> - The ID of the input file for the batch.

**completion_window** <font color="gray">string</font> - The time frame within which the batch should be processed.

**status** <font color="gray">string</font> - The current status of the batch.

**output_file_id** <font color="gray">string</font> - The ID of the file containing the outputs of successfully executed requests.

**error_file_id** <font color="gray">string</font> - The ID of the file containing the outputs of requests with errors.

**created_at** <font color="gray">integer</font> - The Unix timestamp (in seconds) for when the Batch was created.

**in_progress_at** <font color="gray">integer</font> - The Unix timestamp (in seconds) for when the Batch started processing.

**expires_at** <font color="gray">integer</font> - The Unix timestamp (in seconds) for when the Batch will expire.

**finalizing_at** <font color="gray">integer</font> - The Unix timestamp (in seconds) for when the Batch started finalizing.

**completed_at** <font color="gray">integer</font> - The Unix timestamp (in seconds) for when the Batch was completed.

**failed_at** <font color="gray">integer</font> - The Unix timestamp (in seconds) for when the Batch failed.

**expired_at** <font color="gray">integer</font> - The Unix timestamp (in seconds) for when the Batch expired.

**cancelling_at** <font color="gray">integer</font> - The Unix timestamp (in seconds) for when the Batch started cancelling.

**cancelled_at** <font color="gray">integer</font> - The Unix timestamp (in seconds) for when the Batch was cancelled.

**request_counts** <font color="gray">object</font> - The request counts for different statuses within the Batch.

**metadata** <font color="gray">map</font> - Set of 16 key-value pairs that can be attached to an object. This is useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maximum of 512 characters long.

## Request input object

The per-line object of the batch input file

**custom_id**<font color="gray">string</font> - A developer-defined per-request id that is used to match outputs to inputs. Must be unique for each request in a batch.

**method** <font color="gray">string</font> - The HTTP method to be used for the request. Currently only POST is supported.

**url** <font color="gray">string</font> The Kluser.ai API relative URL to be used for the request. Currently, only /v1/completions are supported.

## Request output object

The per-line object of the batch output and error files

<div class="grid" markdown>
<div markdown

**id** <font color="gray">string</font>

**custom_id** <font color="gray">string</font> - A developer-provided per-request id that will be used to match outputs to inputs.

**response** <font color="gray">object or null</font>

  - **status_code** <font color="gray">integer</font> - The HTTP status code of the response
   
  - **request_id** <font color="gray">string</font> - An unique identifier for the API request.

  - **body** <font color="gray">map</font> - The JSON body of the response

**error** <font color="gray">object or null</font> - Requests that failed with a non-HTTP error will contain more information on the cause of the failure.
  
 - **code** <font color="gray">string</font> - A machine-readable error code.
 - **message**<font color="gray"> string</font> A human-readable error message.
</div>
</div>

## Files

## Upload file
`https://api.kluster.ai/v1/files`

Upload a [JSON Lines](https://jsonlines.org/) document to the Kluster.ai endpoint and take a note of the `id` in the response object.

### Request body
---

<div class="grid" markdown>
<div markdown

**file** <font color="gray">file</font> <font color="red">Required</font> - The File object (not file name) to be uploaded.

**purpose** <font color="gray">string</font> <font color="red">Required</font> - The intended purpose of the uploaded file. Use `batch` for the Batch API

</div>
</div>

### Returns
---

The uploaded [**File**](#file-object) object.


## List Files

`get https://api.openai.com/v1/files`

Returns a list of files that belong to the user.

## Query parameters

**purpose** <font color="gray">string</font> <font color="gray">Optional</font> - Only return files with the given purpose.

## Returns

A list of [**File**](#file-object) objects.

## Retrieve file content

`get https://api.kluster.ai/v1/files/{file_id}/content` - Returns the contents of the specified file.

### Path parameters

**file_id** <font color="gray">string</font> <font color="red">Required</font> - The ID of the file to use for this request.

### **Returns**

The output file content matching the specified file ID.

## Delete file

`delete https://api.openai.com/v1/files/{file_id}` - delete a file for the given `file_id`.

### Path parameters
---
**file_id** <font color="gray">string</font>  <font color="red">Required</font> - The ID of the file to use for this request.

### Returns
---
The deletion status of `file_id`

## Retrieve file content

`get https://api.kluster.ai/v1/files/{file_id}` - Returns meta-data about a specific file.

### Path parameters
---
**file_id** <font color="gray">string</font><font color="red">Required</font> - The ID of the file to use for the request.

### Returns
---
The [**File**](#file-object) object matching the specified ID.


## File object

The File object represents a document that has been uploaded to Kluster.ai.

<div class="grid" markdown>
<div markdown>

**id**  <font color="gray">string</font>  - The file identifier referenced in the API endpoints.

**bytes**  <font color="gray">integer</font> - The size of the file in bytes.

**created_at** <font color="gray">integer</font> - The Unix timestamp (in seconds) of when the file was created.

**filename** <font color="gray">string</font> - The name of the file.

**object** <font color="gray">string</font> - The object type which is always `file`.

**purpose** <font color="gray">string</font> - The intended purpose of the file. `batch` and `batch_output` are the supported values

</div>
</div>

## Models
List and describe the various models available in the API. 

## List models
`get https://api.kluster.ai/v1/models` - Lists the currently available models.

### Returns
---
A list of [Model](#model-object) Objects 

## Retrieve model
`get https://api.kluster.ai/v1/models/{model}`

Provides basic information about a specific model.

### Path parameters
---
**model** <font color="gray">string</font> <font color="red">Required</font> - The model ID for this request

### Returns
---

The model object matching the specified model ID.



## Model object

<div class="grid" markdown>
<div markdown>

**id** <font color="gray">string</font> - The model identifier, which can be referenced in the API endpoints.

**created** <font color="gray">integer</font> - The Unix timestamp (in seconds) when the model was created.

**object** <font color="gray">string</font> - The object type, which is always "model".

**owned_by** <font color="gray">string</font> - The organization that owns the model.

</div>
</div>
